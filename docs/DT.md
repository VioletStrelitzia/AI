# 决策树

术语：

- 属性 attribute
- 目标（属性），也即标签属性 target
- 样本 example

把一个问题分成若干个属性，以及一个目标属性。

我们要根据这些属性的取值，来推断目标属性的值。

比如：我去饭馆吃饭，要根据以下属性来判断**是否等待**（目标属性）：

1. 饭馆的类型
2. 座位使用率
3. 今天我的胃口
  ······

我们的目标：构建一棵尽可能小，而又能符合数据集的决策树。

## Decision-Tree-Learning 算法

- 核心思想：划分成可递归的同构子问题。

- 这个算法每次关注构建一个结点，需要的参数是：
  - $RemainAttribute$ 可供当前结点分裂的属性集合。
  - $CurrentExamples$ 当前结点用到的样本集。
  - $ParentExamples$ 父结点用到的样本集。

对于当前结点，我们考虑以下情况：

1. $CurrentExamples$ 为空。

   - 说明当前属性值组合没有观测到样本。
   - 该结点为**叶子结点**。
   - 返回一个缺省值，该值为用于构造该结点的**父结点**的数据集中**占比最大**的目标属性取值。

2. $CurrentExamples$ 全部样本的目标属性 $TargetAttribute$ 取值**全部**都为一个特定的值 $TargetValue$。

   - 该结点为**叶子结点**。
   - 直接返回 $TargetValue$。

3. $RemainAttribute$ 为空。

   - 该结点为**叶子结点**。
   - 返回数据集中**占比最大**的目标属性取值。

4. 上述三个条件都不符合，从 $RemainAttribute$ 里找出**最适合分裂** *（如何衡量？标准是什么？）* 的属性：$CurrentAttribute$。

    已知属性 $CurrentAttribute$ 有 $k$ 个取值，分别为 $Value_k$。

   - 进行**分裂**操作，根据 $CurrentAttribute$ 的取值将 $CurrentExamples$ 分裂成 $k$ 个子样本集 $SubExamples_k$ ，然后对每个子样本集，分别传入孩子结点并进行构建。
    分裂操作：具体来说，就是将 $CurrentAttribute$ 属性取值相同的样本归到一起，结果上就是将原样本集分裂成了 $k$ 个。

## 经典算法

### 1. ID3 算法（Iterative Dichotomiser3）

即迭代二叉树三代，是最早提出的决策树算法。使用信息增益（Information gain）作为分类准则。适用于分类问题。

- 随机变量 $V$ 具有值 $v_i$，分别具有概率  $p_i$ ，则 $V$ 的信息熵为：
$$ Ent(V) = \sum{p_i log_{2}{\frac{1}{p_i}}} = -\sum{p_i log_{2}{p_i}} $$
- 回到决策树中，对于 $Target$ 属性的取值 $v_i$，若在样本容量 $|S|$ 为 $n$ 样本集 $S$ 中含有 $n_i$ 个取值为 $v_i$ 的样本，则 $Target$ 属性在整个样本集的熵是：$$ Ent(S, Target) = -\sum{\frac{n_i}{n}log_{2}{\frac{n_i}{n}}} $$
- 考虑分裂操作，带有 $d$ 个不同值的属性 $A$ 将容量为 $|S|$ 样本集 $S$ 划分为子集 $E_1$，···，$E_d$，每个子集 $E_i$ 容量为 $|E_i|$，假设从样本集中随机挑选的一个样本取属性的第 $i$ 个值的概率为 $ \frac{n_i}{n} $，则样本集从 $A$ 分裂出来之后，剩余的期望熵是：$$  Remainder(S, A) = \sum_{i = 1}^{d}{\frac{|E_i|}{|S|}Ent(E_i, Target)} $$
- 从对属性 $A$ 的分裂获得的信息收益即为熵的期望的减少：$$ IG(A) = Ent(S, Target) - Remainder(S, A) $$
  显然，$IG$ 越大，这个属性分裂的收益就越大。

这样，我们就获得了一种用于获取当前最适合分裂的属性的一种方法。

### 2. C4.5 算法

由 J.Ross Quinlan 在 ID3 的基础上提出，是 ID3 的改进版，用信息增益率作为分类准则。适用于分类、回归问题。

- 在 ID3 的基础上，对属性 $A$（非目标属性），样本集 $S$，以信息增益率作为标准：$$ IGR(S, A) = \frac{IG(S, A)}{Ent(S, A)} $$
- 优点：
  - 产生的分类规则易于理解，准确率较高。
- 缺点：
  - 在构造树的过程中，需要对数据集进行更多的扫描，导致算法的低效。

### 3. CART 算法（Classification and Regression Tree）

分类回归树，用基尼指数作为分裂准则。适用于分类、回归问题。

- 随机变量 $V$ 具有值 $v_i$，分别具有概率  $p_i$ ，则 $V$ 的基尼指数的定义为：
  $$ Gini(V) = 1 - \sum{p_i^2} $$
  基尼指数表示的是样本的不纯度，越大则越不纯，不同分类的样本较多。

- 考虑分裂操作，带有 $d$ 个不同值 $v_1,···, v_d$ 的属性 $A$ 将容量为 $|S|$ 样本集 $S$ 划分为子集 $E_1$，···，$E_d$，每个子集 $E_i$ 容量为 $|E_i|$，假设从样本集中随机挑选的一个样本属性 $A$ 取值为 $v_i$ 的概率为 $ \frac{n_i}{n} $，则样本集从 $A$ 分裂获得的基尼指数 $(GiniIndex)$：
  $$ GI(S, A) = \sum{ \frac{|E_i|}{|S|} Gini(E_i, Target) } $$

## 连续值

处理思想：离散化为一个个区间。
最简单的方法：寻找二分点（注意，为了拟合更多划分，该属性不应该能被移除，以保证后续还可以划分）。

## 决策树剪枝

避免决策树过拟合。

### 1. 预剪枝

### 2. 后剪枝
