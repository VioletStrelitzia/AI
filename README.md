# AI

《人工智能：现代方法》、《机器学习》的学习实践代码和笔记。

## 一、算法

### 1. 启发式搜索（Heuristic Search）

- 关键是启发式函数的选择。
  - 如果启发式函数选的好，那么很快就会收敛到一个解。
  - 如果不好，甚至不如穷举——有可能找不到解。

以八数码（n数码）问题为例。

### 2. 约束满足问题（Constraint Satisfaction Problem, CSP）

#### 2.1. 定义

约束满足问题包含三个成分：

- $X$ **变量**集合 $\{X_1,\dots, X_n\}$。
- $D$ **值域**集合 $\{D_1,\dots, D_n\}$，每个变量都有自己的值域。
- $C$ 描述变量取值的**约束**集合。
  这里的约束是**绝对约束**，即一定要满足的约束。但现实世界的约束还包括**偏好约束**，指出那些解是更喜欢的。包括后者的的问题称为**约束优化问题**（COP）。

CSP 解决的问题可以抽象为从 $X$ 的值域 $D$ 里选取一系列值，并且在这些取值下 $X$ 满足约束 $C$。
CSP 的解是 $X$ 的一个完整赋值。

#### 2.2. CSP 的操作

1. 搜索：
从几种可能性中选择新的变量赋值。

2. 约束传播（推理）：
使用约束来减小一个变量的合法取值范围，从而影响到与此变量有约束关系的其他变量的取值。核心思想：**局部相容性**。

二者可以交替进行，或者在搜索前进行约束传播。

#### 2.3. 局部相容性

1. **结点相容**
   - 定义：单个变量值域中所有取值都满足它的一元约束。

2. **弧相容**
   - 定义：对 $\forall v_i\in D_i$ ，均 $\exists v_{j_0}\in D_j$ 使得 $<v_i, v_{j_0}>$ 满足弧 $<X_i, X_j>$ 的二元约束。
   - 由于 $n$ 元约束都可以抽象成一系列的二元约束，所以通常考虑只含有二元约束的 CSP 问题。

3. **路径相容**
pass

4. **k-相容**
pass

5. **全局约束**
pass

以n皇后问题为例

### 3. 决策树（Decision Tree, DT）

#### 3.1. 概念

- *属性*, $attribute$
- *目标*（属性），也即标签属性 $target$
- *样本*, $example$

把一个问题分成若干个属性，以及一个目标属性。

我们要根据这些属性的取值，来推断目标属性的值。

比如：我去饭馆吃饭，要根据以下属性来判断**是否等待**（目标属性）：

1. 饭馆的类型
2. 座位使用率
3. 今天我的胃口
  ······

我们的目标：构建一棵尽可能小，而又能符合数据集的决策树。

#### 3.2. Decision-Tree-Learning 算法

- 核心思想：划分成可递归的同构子问题。

- 这个算法每次关注构建一个结点，需要的参数是：
  - $RemainAttribute$ 可供当前结点分裂的属性集合。
  - $CurrentExamples$ 当前结点用到的样本集。
  - $ParentExamples$ 父结点用到的样本集。

对于当前结点，我们考虑以下情况：

1. $CurrentExamples$ 为空。

   - 说明当前属性值组合没有观测到样本。
   - 该结点为**叶子结点**。
   - 返回一个缺省值，该值为用于构造该结点的**父结点**的数据集中**占比最大**的目标属性取值。

2. $CurrentExamples$ 全部样本的目标属性 $TargetAttribute$ 取值**全部**都为一个特定的值 $TargetValue$。

   - 该结点为**叶子结点**。
   - 直接返回 $TargetValue$。

3. $RemainAttribute$ 为空。

   - 该结点为**叶子结点**。
   - 返回数据集中**占比最大**的目标属性取值。

4. 上述三个条件都不符合，从 $RemainAttribute$ 里找出**最适合分裂** *（如何衡量？标准是什么？）* 的属性：$CurrentAttribute$。

    已知属性 $CurrentAttribute$ 有 $k$ 个取值，分别为 $Value_k$。

   - 进行**分裂**操作，根据 $CurrentAttribute$ 的取值将 $CurrentExamples$ 分裂成 $k$ 个子样本集 $SubExamples_k$ ，然后对每个子样本集，分别传入孩子结点并进行构建。
    分裂操作：具体来说，就是将 $CurrentAttribute$ 属性取值相同的样本归到一起，结果上就是将原样本集分裂成了 $k$ 个。

#### 3.3. 经典分裂算法

##### 1. ID3 算法（Iterative Dichotomiser3）

即迭代二叉树三代，是最早提出的决策树算法。使用信息增益（Information gain）作为分类准则。适用于分类问题。

- 随机变量 $V$ 具有值 $v_i$，分别具有概率  $p_i$ ，则 $V$ 的信息熵为：
$$ Ent(V) = \sum{p_i log_{2}{\frac{1}{p_i}}} = -\sum{p_i log_{2}{p_i}} $$
- 回到决策树中，对于 $Target$ 属性的取值 $v_i$，若在样本容量 $|S|$ 为 $n$ 样本集 $S$ 中含有 $n_i$ 个取值为 $v_i$ 的样本，则 $Target$ 属性在整个样本集的熵是：$$ Ent(S, Target) = -\sum{\frac{n_i}{n}log_{2}{\frac{n_i}{n}}} $$
- 考虑分裂操作，带有 $d$ 个不同值的属性 $A$ 将容量为 $|S|$ 样本集 $S$ 划分为子集 $E_1$，···，$E_d$，每个子集 $E_i$ 容量为 $|E_i|$，假设从样本集中随机挑选的一个样本取属性的第 $i$ 个值的概率为 $ \frac{n_i}{n} $，则样本集从 $A$ 分裂出来之后，剩余的期望熵是：$$  Remainder(S, A) = \sum_{i = 1}^{d}{\frac{|E_i|}{|S|}Ent(E_i, Target)} $$
- 从对属性 $A$ 的分裂获得的信息收益即为熵的期望的减少：$$ IG(A) = Ent(S, Target) - Remainder(S, A) $$
  显然，$IG$ 越大，这个属性分裂的收益就越大。

这样，我们就获得了一种用于获取当前最适合分裂的属性的一种方法。

##### 2. C4.5 算法

由 J.Ross Quinlan 在 ID3 的基础上提出，是 ID3 的改进版，用信息增益率作为分类准则。适用于分类、回归问题。

- 在 ID3 的基础上，对属性 $A$（非目标属性），样本集 $S$，以信息增益率作为标准：$$ IGR(S, A) = \frac{IG(S, A)}{Ent(S, A)} $$
- 优点：
  - 产生的分类规则易于理解，准确率较高。
- 缺点：
  - 在构造树的过程中，需要对数据集进行更多的扫描，导致算法的低效。

##### 3. CART 算法（Classification and Regression Tree）

分类回归树，用基尼指数作为分裂准则。适用于分类、回归问题。

- 随机变量 $V$ 具有值 $v_i$，分别具有概率  $p_i$ ，则 $V$ 的基尼指数的定义为：
  $$ Gini(V) = 1 - \sum{p_i^2} $$
  基尼指数表示的是样本的不纯度，越大则越不纯，不同分类的样本较多。

- 考虑分裂操作，带有 $d$ 个不同值 $v_1,···, v_d$ 的属性 $A$ 将容量为 $|S|$ 样本集 $S$ 划分为子集 $E_1$，···，$E_d$，每个子集 $E_i$ 容量为 $|E_i|$，假设从样本集中随机挑选的一个样本属性 $A$ 取值为 $v_i$ 的概率为 $ \frac{n_i}{n} $，则样本集从 $A$ 分裂获得的基尼指数 $(GiniIndex)$：
  $$ GI(S, A) = \sum{ \frac{|E_i|}{|S|} Gini(E_i, Target) } $$

#### 3.4. 连续值

处理思想：离散化为一个个区间。
最简单的方法：寻找二分点（注意，为了拟合更多划分，该属性不应该能被移除，以保证后续还可以划分）。

#### 3.5. 决策树剪枝

避免决策树过拟合。

##### 1. 预剪枝

##### 2. 后剪枝

### 4. 遗传算法（Genetic Algorithm, GA）

#### 4.1. 基本算法

1. 初始化种群。
2. 计算适应度（$Fitness$），根据适应度来进行自然选择（$Selection$）。
3. 种群进行繁殖、突变，生成新一代种群。
4. 重复2，3直到达到终止条件。

#### 4.2. 选择算法

在遗传算法中，选择染色体进行交叉互换的算法通常与交叉算子本身是独立的。选择染色体的过程称为选择操作（$Selection$），而交叉互换是交叉操作（$Crossover$）。选择操作的目的是确定哪些个体将参与下一代的交叉和变异过程。以下是一些常用的选择算法：

1. **轮盘赌选择（Roulette Wheel Selection）**：
   - 每个个体被选中的概率与其适应度成正比。适应度越高的个体被选中的机会越大。

2. **锦标赛选择（Tournament Selection）**：
   - 随机选择一小部分个体，然后从中选择适应度最高的个体。

3. **精英选择（Elitism）**：
   - 保证每一代中最优秀的个体直接被复制到下一代。

4. **随机选择（Random Selection）**：
   - 完全随机地选择个体，不考虑适应度。

5. **基于排名的选择（Rank Selection）**：
   - 个体被选中的概率与其排名成正比，而不是适应度值。

6. **适应度比例选择（Fitness Proportionate Selection）**：
   - 类似于轮盘赌选择，但个体被选中的概率与其适应度成正比。

7. **截断选择（Truncation Selection）**：
   - 只选择适应度最高的一定比例的个体。

8. **博尔达计数选择（Borda Count Selection）**：
   - 每个个体根据其排名获得分数，然后根据总分选择个体。

9. **非支配排序选择（Non-dominated Sorting Selection）**：
   - 用于多目标优化问题，根据个体在非支配排序中的位置来选择。

10. **基于密度的选择（Density-based Selection）**：
    - 考虑个体的适应度和它们在解空间中的分布密度。

#### 4.3. 染色体交叉互换算法

1. **单点交叉（Single-point crossover）**
   - 在两条染色体上随机选择一个位置点，然后交换该点右侧的部分，生成两个不同的子染色体。

2. **两点交叉（Two-points crossover）**
   - 在个体染色体中随机设置两个交叉点，然后交换这两个点之间的部分染色体。

3. **多点交叉（Multi-point crossover）**
   - 在个体染色体中随机设置多个交叉点，然后进行基因交换。如果只选择了一个交叉点，那么多点交叉就变成了单点交叉。

4. **部分匹配交叉（Partially-matched crossover，PMX）**
   - 保证每个染色体中的基因仅出现一次，适用于旅行商问题（TSP）或其他排序问题编码。通过随机选择两个交叉点确定交叉区域，并在交叉区域外对重复基因应用匹配关系消除冲突。

5. **均匀交叉（Uniform crossover）**
   - 两个染色体的索引i处的基因以交换概率pS进行交换，可以更好地搜索设计空间，同时保持良好的信息交换。

6. **顺序交叉（Order Crossover，OX）**
   - 在两个父代染色体中随机选择起始和结束位置，将父代染色体1该区域内的基因复制到子代1相同位置上，再在父代染色体2上将子代1中缺少的基因按照顺序填入。

7. **基于位置的交叉（Position-based Crossover，PBX）**
   - 在两个父代染色体中随机选择几个位置，位置可以不连续，将父代染色体1这些位置上的基因复制到子代1相同位置上，再在父代染色体2上将子代1中缺少的基因按照顺序填入。

8. **基于顺序的交叉（Order-Based Crossover，OBX）**
   - 在两个父代染色体中随机选择几个位置，位置可以不连续，先在父代染色体2中找到父代染色体1被选中基因的位置，再用父代染色体2中其余的基因生成子代，并保证位置对应，将父代染色体1中被选择的基因按顺序放入子代剩余位置中。

9. **循环交叉（Cycle Crossover，CX）**
   - 在某个父代上随机选择1个基因，然后找到另一个父代相应位置上的基因编号，再回到第一个父代找到同编号的基因的位置，重复先前工作，直至形成一个环，环中的所有基因的位置即为最后选中的位置。

10. **子路径交叉交叉（Subtour Exchange Crossover，SEX）**
    - 在某个父代上选择1组基因，在另一父代上找到这些基因的位置，保持未选中基因不变，按选中基因的出现顺序，交换两父代染色体中基因的位置，一次生成两个子代。

在选择操作之后，交叉操作会从被选中的个体中随机或根据某种策略选择两个或多个个体进行交叉，以产生新的后代。交叉算子的选择（如单点交叉、两点交叉等）会影响新后代的基因组合方式，但不直接影响哪些个体被选中进行交叉。

### 5. 最近邻算法（Nearest Neighbor Search, NNS）

#### 5.1. 暴力查找（Brute-force Search）

最简单的最邻近搜索方法，遍历整个点集。

- 适用于小规模数据集。
- 大规模数据集或高维空间效率较低。

#### 5.2. KD树（k-D Tree）

KD树是一种用于多维空间数据的快速检索的数据结构，通过递归地将空间划分为更小的区域来实现对点的快速查找和邻近搜索。

#### 5.3. 局部敏感散列（Locality Sensitive Hashing, LSH）

这是一种基于对点进行操作的距离度量将空间中的点分组为“桶”的技术，在所选度量下彼此靠近的点以高概率映射到同一个桶。

#### 5.4. 球树（Ball Tree）

球树是一种数据结构，用于多维空间中点的快速检索，通过构建一系列嵌套的超球体来组织数据点。

#### 5.5. M树和MVP树

M树是一种平衡的树结构，用于多维空间数据的索引，而MVP树是M树的变种，用于近似最近邻搜索。

#### 5.6. 投影径向搜索（Projected Radial Search）

在数据是几何点的密集3D地图的特殊情况下，传感技术的投影几何可用于显着简化搜索问题。

#### 5.7. 矢量近似文件（Vector Approximation Files）

这种方法涉及到使用矢量量化技术来近似表示数据点，以便于快速检索。

#### 5.8. k-最近邻（k-Nearest Neighbors, KNN）

k-最近邻搜索识别查询的前k个最近邻，这种技术通常用于预测分析，以根据其邻居的共识来估计或分类一个点。

#### 5.9. 近似最近邻（Approximate Nearest Neighbor）

在某些应用程序中，检索最近邻居的“正确猜测”可能是可以接受的，这时可以使用一种算法，该算法不能保证在每种情况下都返回实际的最近邻居，以换取提高速度或节省内存。

#### 5.10. 最近邻距离比（Nearest Neighbor Distance Ratio）

不是将阈值应用于从原始点到挑战者邻居的直接距离，而是根据与前一个邻居的距离的比率来应用阈值。

### 6. 降维算法

机器学习中的降维算法主要用于减少数据特征的数量，同时尽可能保留重要信息。常见的降维算法包括：

#### 6.1. 线性降维

1. **主成分分析 (PCA)**
   - 通过线性变换将数据投影到低维空间，保留最大方差。

2. **线性判别分析 (LDA)**
   - 在降维的同时最大化类间距离，最小化类内距离，适用于分类任务。

3. **因子分析 (FA)**
   - 假设数据由潜在变量生成，通过估计这些变量来降维。

4. **多维缩放 (MDS)**
   - 保持样本间的距离关系，适用于高维数据的可视化。

5. **非负矩阵分解 (NMF)**
   - 将数据矩阵分解为非负矩阵的乘积，适用于非负数据。

#### 6.2. 非线性降维

1. **t-分布邻域嵌入 (t-SNE)**
   - 保持高维空间中样本的局部结构，常用于可视化。

2. **均匀流形逼近与投影 (UMAP)**
   - 保持局部和全局结构，计算效率高于t-SNE。

3. **自编码器 (Autoencoder)**
   - 通过神经网络学习数据的低维表示，包含编码器和解码器两部分。

4. **等距映射 (Isomap)**
   - 保持测地距离，适用于非线性流形数据。

5. **局部线性嵌入 (LLE)**
   - 保持局部线性关系，适用于非线性流形数据。

6. **核主成分分析 (Kernel PCA)**
   - 使用核技巧将数据映射到高维空间后进行PCA，适用于非线性数据。

#### 6.3. 其他方法

1. **随机投影 (Random Projection)**
   - 通过随机矩阵将数据投影到低维空间，计算效率高。

2. **独立成分分析 (ICA)**
   - 假设数据由独立成分生成，通过分离这些成分来降维。

3. **拉普拉斯特征映射 (Laplacian Eigenmaps)**
   - 利用图的拉普拉斯矩阵进行降维，保持局部结构。

4. **局部保持投影 (LPP)**
   - 保持局部结构，适用于流形学习。

#### 6.4. 选择依据

- **数据特性**：线性或非线性。
- **任务需求**：分类、回归或可视化。
- **计算资源**：算法的时间和空间复杂度。

这些算法各有优劣，需根据具体问题选择合适的降维方法。
